{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f111fb89",
   "metadata": {},
   "source": [
    "## Read and tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e655e6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d8f4c168f3d8923e\n",
      "Reusing dataset csv (/users/phd/kpawan/.cache/huggingface/datasets/csv/default-d8f4c168f3d8923e/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c281ffdf97374159a37d0f97178cff0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /users/phd/kpawan/.cache/huggingface/datasets/csv/default-d8f4c168f3d8923e/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-d1f6995c61059c99.arrow\n",
      "Loading cached processed dataset at /users/phd/kpawan/.cache/huggingface/datasets/csv/default-d8f4c168f3d8923e/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-b325f37cdfa3cd0f.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "#raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets = load_dataset('csv', data_files={'train': ['train.csv'], 'eval': 'test.csv'})\n",
    "\n",
    "\n",
    "# checkpoint = \"bert-base-uncased\"\n",
    "checkpoint = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3fc7114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attention_mask', 'input_ids', 'label', 'text']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb2067be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attention_mask', 'input_ids', 'labels']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "085ed81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=32, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"eval\"], batch_size=8, collate_fn=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f218deb7",
   "metadata": {},
   "source": [
    "## Test batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d65a7b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': torch.Size([32, 47]),\n",
       " 'input_ids': torch.Size([32, 47]),\n",
       " 'labels': torch.Size([32])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b96fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6fd34",
   "metadata": {},
   "source": [
    "## compute_metric function for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df1caad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76427d06",
   "metadata": {},
   "source": [
    "## Inherit Trainer and Override compute_loss to change the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b0e4d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from torch import nn\n",
    "\n",
    "class MultilabelTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        cuda0 = torch.device('cuda:0')\n",
    "        pos_weight = torch.ones([self.model.config.num_labels], device=cuda0)\n",
    "#         pos_weight[16] = 2.0\n",
    "#         loss_fct = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        loss_fct = nn.BCEWithLogitsLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels),\n",
    "                        torch.nn.functional.one_hot(labels, num_classes=self.model.config.num_labels).float().view(-1, self.model.config.num_labels))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7382c18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6799b85",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf074692",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 8000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 315\n",
      "/data/kpawan/anaconda3/envs/st/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 07:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.947050</td>\n",
       "      <td>0.744203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.617030</td>\n",
       "      <td>0.826647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.544373</td>\n",
       "      <td>0.842841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.558868</td>\n",
       "      <td>0.845418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.559438</td>\n",
       "      <td>0.844682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2717\n",
      "  Batch size = 128\n",
      "/data/kpawan/anaconda3/envs/st/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2717\n",
      "  Batch size = 128\n",
      "/data/kpawan/anaconda3/envs/st/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2717\n",
      "  Batch size = 128\n",
      "/data/kpawan/anaconda3/envs/st/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2717\n",
      "  Batch size = 128\n",
      "/data/kpawan/anaconda3/envs/st/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2717\n",
      "  Batch size = 128\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=315, training_loss=0.7400496709914435, metrics={'train_runtime': 455.9329, 'train_samples_per_second': 87.732, 'train_steps_per_second': 0.691, 'total_flos': 1623083804017536.0, 'train_loss': 0.7400496709914435, 'epoch': 5.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=19)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"test-trainer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "# trainer = MultilabelTrainer(\n",
    "#     model,\n",
    "#     training_args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets[\"eval\"],\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"eval\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b337afe1",
   "metadata": {},
   "source": [
    "## Get model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dbadb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2717\n",
      "  Batch size = 128\n",
      "/data/kpawan/anaconda3/envs/st/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2717, 19) (2717,)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"eval\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d450ec9",
   "metadata": {},
   "source": [
    "## Compute different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afd8bbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8446816341553184}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "metric = load_metric(\"accuracy\",\"\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dfbe680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8446816341553184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 46,   0,   0,   0,   1,   0,   0,   0,   0,   1,   0,   1,   0,\n",
       "          0,   0,   1,   1,   0,   0],\n",
       "       [  0,  42,   0,   0,   0,   0,   0,   0,   0,   1,   0,   0,   0,\n",
       "          0,   0,   1,   2,   1,   0],\n",
       "       [  0,   0, 112,   1,   3,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          1,   0,   1,  16,   0,   0],\n",
       "       [  0,   0,   0, 181,   0,   3,   0,   0,   0,   1,   0,   0,   0,\n",
       "          0,   0,   0,   9,   0,   0],\n",
       "       [  0,   0,   5,   0, 126,   0,   3,   0,   0,   0,   0,   1,   0,\n",
       "          4,   4,   1,   6,   0,   0],\n",
       "       [  0,   0,   0,   4,   0,  91,   0,   2,   0,   5,   0,   0,   0,\n",
       "          0,   0,   2,   4,   0,   0],\n",
       "       [  0,   1,   0,   0,   1,   0,  36,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   1,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   2,   0,  15,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   5,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   1,   0,   0],\n",
       "       [  0,   0,   0,   4,   0,   5,   0,   0,   0, 189,   0,   0,   3,\n",
       "          0,   0,   0,  10,   0,   0],\n",
       "       [  1,   0,   0,   0,   0,   0,   0,   0,   0,   1,  24,   1,   1,\n",
       "          0,   0,   0,   4,   0,   0],\n",
       "       [  1,   0,   0,   0,   4,   0,   1,   1,   0,   0,   5, 141,   0,\n",
       "          0,   1,   1,   7,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1, 275,\n",
       "          1,   0,   0,  13,   0,   1],\n",
       "       [  0,   0,   0,   0,   3,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        199,   1,   0,   7,   0,   0],\n",
       "       [  0,   0,   0,   0,   1,   1,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0, 184,   0,  15,   0,   0],\n",
       "       [  0,   0,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          1,   0, 116,   5,   0,   0],\n",
       "       [  5,   3,  15,   5,  16,   8,   2,   5,   0,  14,  12,  12,  18,\n",
       "         23,  24,  15, 251,  13,  13],\n",
       "       [  0,   0,   0,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          2,   0,   1,   3, 127,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   7,\n",
       "          0,   0,   0,   2,   0, 140]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "print(f1_score(predictions.label_ids, preds, average='micro'))\n",
    "confusion_matrix(predictions.label_ids, preds, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st",
   "language": "python",
   "name": "st"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
